{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import time\n",
    "from flair.data import Sentence\n",
    "from flair.data import Token\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "\n",
    "# embedding size of fasttext models\n",
    "d_model = 300 \n",
    "\n",
    "# select language\n",
    "language_in = 'de'\n",
    "language_out = 'en' \n",
    "\n",
    "# load word embedding models\n",
    "embedding_model_en = WordEmbeddings('en')\n",
    "embedding_model_de = WordEmbeddings('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"\n",
    "vocab_in = joblib.load(path+f'vocab_{language_in}.data')\n",
    "vocab_out = joblib.load(path+f'vocab_{language_out}.data')\n",
    "\n",
    "seq_len_encoder = vocab_in[\"max_sentence_len\"] + 1\n",
    "seq_len_decoder = vocab_out[\"max_sentence_len\"] + 1\n",
    "vocab_size = vocab_out[\"vocab_size\"] + 1 # + <EOS>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = torch.load(path+f'transformer.pt')\n",
    "transformer.eval()\n",
    "print(\"model weights:\",sum(p.numel() for p in transformer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_mask = [2*i for i in range(d_model//2)]\n",
    "cosine_mask = [2*i+1 for i in range(d_model//2)]\n",
    "i = torch.tensor([i for i in range(d_model//2)]).to(device).to(dtype)\n",
    "\n",
    "# pre compute positional encodings for 50 tokens\n",
    "positional_encodings = []\n",
    "for position in range(50):\n",
    "    \n",
    "    sine = torch.sin( position / ( 10000 ** (2*i/d_model) ) )\n",
    "    cosine = torch.cos( position / ( 10000 ** (2*i/d_model) ) )\n",
    "    \n",
    "    position_enc = torch.zeros(d_model).to(device)\n",
    "    position_enc[sine_mask] = sine\n",
    "    position_enc[cosine_mask] = cosine\n",
    "\n",
    "    positional_encodings.append(position_enc)\n",
    "\n",
    "# return pre computed pos encoding\n",
    "def positional_encoding(sentence):\n",
    "    num_tokens = len(sentence)\n",
    "    return positional_encodings[0:num_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    # constructed sentence\n",
    "    sentence_translated = \"\"\n",
    "\n",
    "    sentence_de = Sentence(sentence,language_code='de')\n",
    "    # add <EOS> token\n",
    "    sentence_de.add_token(\"<EOS>\")\n",
    "    # embed\n",
    "    embedding_model_de.embed(sentence_de)\n",
    "    # pos encoding\n",
    "    pos_enc = positional_encoding(sentence_de)\n",
    "    # add embedding and pos enc\n",
    "    emb_in = torch.stack([token.embedding + pos_enc[i] for i,token in enumerate(sentence_de)])\n",
    "    # add padding\n",
    "    seq_len_in = emb_in.shape[0]\n",
    "    padding_length_in = vocab_in[\"max_sentence_len\"] + 1 - seq_len_in            \n",
    "    padding = torch.zeros(padding_length_in, d_model).to(device)\n",
    "\n",
    "    x_in = torch.unsqueeze(torch.cat([emb_in, padding], dim=0),dim=0)\n",
    "\n",
    "    # starting point of ouput\n",
    "    sentence_en = Sentence(' ')\n",
    "    sentence_en.add_token('<SOS>')\n",
    "\n",
    "    done = False\n",
    "    i= 0\n",
    "    while not done and i < vocab_out[\"max_sentence_len\"] + 1:\n",
    "        #print(sentence_en)\n",
    "\n",
    "        # embed\n",
    "        embedding_model_en.embed(sentence_en)\n",
    "        # pos encoding\n",
    "        pos_enc = positional_encoding(sentence_en)\n",
    "        # add embedding and pos enc\n",
    "        emb_out = torch.stack([token.embedding + pos_enc[i] for i,token in enumerate(sentence_en)])\n",
    "        # add padding\n",
    "        seq_len_out = emb_out.shape[0]\n",
    "        padding_length_out = vocab_out[\"max_sentence_len\"] + 1 - seq_len_out            \n",
    "        padding = torch.zeros(padding_length_out, d_model).to(device)\n",
    "        x_out = torch.unsqueeze(torch.cat([emb_out, padding], dim=0),dim=0)\n",
    "        out = transformer(x_in, x_out)\n",
    "\n",
    "        # reshape to batch_size, seq, vocab\n",
    "        out = torch.reshape(out, (1, seq_len_decoder,vocab_size))\n",
    "        out = F.softmax(out, dim=2) \n",
    "        max_index = torch.argmax(out[0], dim=1).cpu().numpy()\n",
    "        # get relevant position\n",
    "        token_index = max_index[i]\n",
    "\n",
    "        token = vocab_out[token_index]\n",
    "        if token == \"<EOS>\":\n",
    "            done = True\n",
    "        else:\n",
    "            sentence_translated += token + \" \"\n",
    "            sentence_en.add_token(token)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return sentence_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_german = ['Ich habe dich einst gewarnt, aber du hast nicht hören wollen.',\n",
    "                          'Ich freue mich, dass ausnahmsweise mal jemand meiner Meinung ist.',\n",
    "                          'Ich kann dir das Geld, das du mir geliehen hast, nicht zurückzahlen.',\n",
    "                          'Das Essen war schrecklich, aber ich habe mich nicht beklagt.',\n",
    "                          'Diese Behauptungen entbehren einer wissenschaftlichen Grundlage.',\n",
    "                          'Was hältst du davon, wenn wir heute Abend zur Abwechslung mal draußen essen?',\n",
    "                          'Ich habe von ihm gehört, aber ich kenne ihn nicht persönlich.',\n",
    "                          'Ich glaube, wir haben uns den Falschen für die Stelle gesucht.',\n",
    "                          'Ich möchte glauben, dass wir aus unseren Fehlern lernen.',\n",
    "                          'In Kalifornien haben die meisten Häuser einen Rohbau aus Holz.',\n",
    "                          'Die Preise sind doppelt so hoch wie vor zehn Jahren']\n",
    "\n",
    "for sentence in train_sentences_german:\n",
    "    print(f'In: {sentence} | Out: {translate(sentence)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test sentences\n",
    "test_sentences_german = ['Ich bin doppelt so alt wie vor zehn Jahren.', 'Ich bin sehr begeistert von deinem Talent.', 'Das ist ein Problem, das wir lösen können.',\n",
    "                        'Sie genießen den sanften Wind am Meer.', 'Das ist wirklich das beste Eis der Stadt.',\n",
    "                        'Wie viele Menschen gibt es auf der Welt?','Mir fällt kein neuer Satz ein.',\n",
    "                        'Mein neues Modell ist ganz gut, obwohl es Schwierigkeiten gab.',\n",
    "                        'Dieses Weihnachten ist wie kein anderes zuvor.','Wie viele Häuser stehen auf der anderen Straßenseite?',\n",
    "                        'Morgen ist schon wieder Montag.','Das Fahrrad hat zwei Räder und eine Klingel und ist schnell.',\n",
    "                        'Augen auf, sonst fällst du auf den Boden!', 'Die Polizei verhaftet den Verbrecher und setzt ihn in das Auto.',\n",
    "                        'Ich habe zu wenig Sätze zum Trainieren des Modells.','Lasst die Frau auf dem Auto tanzen.',\n",
    "                        'Es gibt so viele Möglichkeiten, aber ich bin nicht kreativ genug.','Die Küche muss renoviert werden, sagte sie.',\n",
    "                        'Bitte, mach mir das Leben nicht schwer.',\n",
    "                        'Heute muss ich zum Zahnarzt, obwohl es Sonntag ist.','Wie viele Einheiten bilden ein Dutzend?',\n",
    "                        'Was für einen Stift braucht man für das Dokument?','Hände hoch, oder die Hose runter.']\n",
    "\n",
    "for sentence in test_sentences_german:\n",
    "    print(f'In: {sentence} | Out: {translate(sentence)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
