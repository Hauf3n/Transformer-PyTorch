{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from flair.data import Sentence\n",
    "from flair.data import Token\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "\n",
    "# embedding size of fasttext models\n",
    "d_model = 300 \n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# select language\n",
    "language_in = 'de'\n",
    "language_out = 'en' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_DataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path, language_in, language_out):#D:\\Transformer\\dataset.data\n",
    "        super().__init__()\n",
    "        self.language_in = language_in\n",
    "        self.language_out = language_out\n",
    "\n",
    "        # load dat and vocab\n",
    "        self.data = joblib.load(path+f'{language_in}_to_{language_out}.data')\n",
    "        self.vocab_in = joblib.load(path+f'vocab_{language_in}.data')\n",
    "        self.vocab_out = joblib.load(path+f'vocab_{language_out}.data')\n",
    "\n",
    "        # zero padding info\n",
    "        self.seq_len_in = self.vocab_in[\"max_sentence_len\"] + 1 # additional <SOS>/<EOS> token\n",
    "        self.seq_len_out = self.vocab_out[\"max_sentence_len\"] + 1 # additional <SOS>/<EOS> token\n",
    "\n",
    "        # precompute padding\n",
    "        self.precompute_padding = torch.zeros(1, d_model).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return format: input_encoding, output_encoding, target\n",
    "\n",
    "        x_in, x_out, target = self.data[index]\n",
    "        x_in = torch.tensor(x_in).to(device)\n",
    "        x_out = torch.tensor(x_out).to(device)\n",
    "        # zero padding\n",
    "\n",
    "        seq_len_in = x_in.shape[0]\n",
    "        padding_length_in = self.seq_len_in - seq_len_in\n",
    "\n",
    "        seq_len_out = x_out.shape[0]\n",
    "        padding_length_out = self.seq_len_out - seq_len_out\n",
    "\n",
    "        #padding_in = torch.zeros(padding_length_in, d_model).to(device)\n",
    "        #padding_out = torch.zeros(padding_length_out, d_model).to(device)\n",
    "        padding_in = self.precompute_padding.repeat([padding_length_in,1])\n",
    "        padding_out = self.precompute_padding.repeat([padding_length_out,1])\n",
    "\n",
    "        x_in = torch.cat([x_in, padding_in], dim=0)\n",
    "        x_out = torch.cat([x_out, padding_out], dim=0)\n",
    "\n",
    "        # padding target <EOS> as target for padding?\n",
    "        target_len = len(target)\n",
    "        padding_length = self.seq_len_out - target_len\n",
    "        padding_index = target[-1] # <EOS>\n",
    "\n",
    "        target = target + [ padding_index for i in range(padding_length)]\n",
    "        target = torch.tensor(target).to(device)\n",
    "\n",
    "        return x_in, x_out, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"D:\\Transformer\\\\\"\n",
    "dataset = Language_DataSet(path, language_in, language_out)\n",
    "dl = DataLoader(dataset, batch_size=32,num_workers=4, shuffle=True)\n",
    "if __name__ == '__main__':\n",
    "    for eps_data in dl:\n",
    "        x_in, x_out, target = eps_data\n",
    "        print(x_in)\n",
    "        print(x_out)\n",
    "        print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Implementation\n",
    "## Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, seq_len, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dimensions = dimensions\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, dimensions * heads * 3)\n",
    "        self.final_linear = nn.Linear(self.heads * self.dimensions, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # reshape for linear qkv layer\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # compute q,v,k for every head\n",
    "        qkv = self.qkv(x) # (seq * batch, heads * 3 * dimensions)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * 3 * self.dimensions))\n",
    "        # split into heads and seperate q, k, v in different dims\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads, 3, self.dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        qkv = qkv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # extract q, k, v\n",
    "        q = qkv[:,:,:,0,:]\n",
    "        k = qkv[:,:,:,1,:]\n",
    "        v = qkv[:,:,:,2,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        k = torch.reshape(k, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.dimensions).to(device).to(dtype))\n",
    "        # optional masking\n",
    "        if mask is not None:\n",
    "            qk[mask == 1] = float('-inf')\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)     \n",
    "        \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, self.seq_len, self.dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * self.dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * self.seq_len, self.heads * self.dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, seq_len, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, seq_len, attention_dimension)\n",
    "        self.layer_norm_1 = nn.LayerNorm([seq_len,d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([seq_len,d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z = self.self_attention(x)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_1, (batch_size*self.seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + ff_out\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_1(residual_2)\n",
    "        \n",
    "        return norm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len, attention_dimensions):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        \n",
    "        # stacked encoder cells\n",
    "        encoder_cells = [ Encoder_Cell(heads, seq_len, attention_dimensions).to(device) for i in range(cells)]\n",
    "        self.encode = nn.Sequential(*encoder_cells)\n",
    "        \n",
    "        # key and value output of encoder\n",
    "        self.kv = nn.Linear(d_model, attention_dimensions * heads * 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encoding shape: batch_size, seq_len, d_model\n",
    "        encoding = self.encode(x) \n",
    "        \n",
    "        # reshape to feed into linear kv layer\n",
    "        encoding = torch.reshape(encoding, (batch_size * self.seq_len, d_model))\n",
    "        \n",
    "        # apply linear\n",
    "        kv = self.kv(encoding)\n",
    "        # reshape back\n",
    "        kv = torch.reshape(kv, (batch_size, self.seq_len, self.attention_dimensions * self.heads * 2))\n",
    "        \n",
    "        # seperate k and v\n",
    "        kv = torch.reshape(kv, (batch_size, self.seq_len, self.heads, 2, self.attention_dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        kv = kv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # split k, v\n",
    "        k = kv[:,:,:,0,:]\n",
    "        v = kv[:,:,:,1,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        k = torch.reshape(k, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        \n",
    "        return k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, seq_len, attention_dimensions):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        \n",
    "        self.q = nn.Linear(d_model, attention_dimensions * heads).to(device)\n",
    "        self.final_linear = nn.Linear(heads * attention_dimensions, d_model).to(device)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch_size, seq_len, d_model\n",
    "        # encoder k/v shape: batch_size*heads, seq_len, attention_dimensions\n",
    "        \n",
    "        # reshape for linear q layer\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # compute q for every head\n",
    "        q = self.q(x) # (seq * batch, heads * attention_dimensions)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        q = torch.reshape(q, (batch_size, self.seq_len, self.heads * self.attention_dimensions))\n",
    "        # split into heads \n",
    "        q = torch.reshape(q, (batch_size, self.seq_len, self.heads, self.attention_dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        q = q.permute(0,2,1,3)\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(encoder_k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.attention_dimensions).to(device).to(dtype))\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)     \n",
    "        \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, encoder_v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, self.seq_len, self.attention_dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * self.attention_dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * self.seq_len, self.heads * self.attention_dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        return z     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Cell(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, seq_len, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # construct decoder mask # MASK CORRECT?????\n",
    "        a = np.triu(np.ones((seq_len,seq_len)), k=1) \n",
    "        mask = torch.unsqueeze(torch.tensor(a).to(device).long(),dim=0)\n",
    "        self.mask = mask.repeat(batch_size*heads,1,1)\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, seq_len, attention_dimension).to(device)\n",
    "        self.enc_dec_attention = Encoder_Decoder_Attention(heads, seq_len, attention_dimension).to(device)\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm([seq_len,d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([seq_len,d_model])\n",
    "        self.layer_norm_3 = nn.LayerNorm([seq_len,d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z_1 = self.self_attention(x, self.mask)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z_1\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        z_2 = self.enc_dec_attention(norm_1, encoder_k, encoder_v)\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + z_2\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_2(residual_2)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_2, (batch_size*self.seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        # 3rd residual\n",
    "        residual_3 = norm_2 + ff_out\n",
    "        # 3rd norm\n",
    "        norm_3 = self.layer_norm_3(residual_3)\n",
    "        \n",
    "        return norm_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len, attention_dimensions, vocab_size):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # stacked encoder cells\n",
    "        self.decoder_cells = [ Decoder_Cell(heads, seq_len, attention_dimensions).to(device) for i in range(cells)]\n",
    "        \n",
    "        # output layer and then softmax\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        \n",
    "        for decoder_cell in self.decoder_cells:\n",
    "            x = decoder_cell(x,encoder_k, encoder_v)\n",
    "            \n",
    "        # reshape for linear\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # feed in final layer\n",
    "        x = self.final_linear(x)\n",
    "        \n",
    "        # softmax over vocab_size\n",
    "        #softmax = F.softmax(x, dim=1) maybe do, when input to cross entropy loss\n",
    "        #return softmax\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len_enc, seq_len_dec, attention_dimensions, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(cells, heads, seq_len_enc, attention_dimensions).to(device)\n",
    "        self.decoder = Decoder(cells, heads, seq_len_dec, attention_dimensions, vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x_encoder, x_decoder):\n",
    "        # x_in shape: batch_size, seq_len_in, d_model\n",
    "        # x_out shape: batch_size, seq_len_out, d_model\n",
    "        \n",
    "        encoder_k, encoder_v = self.encoder(x_encoder)\n",
    "        out = self.decoder(x_decoder, encoder_k, encoder_v)\n",
    "        \n",
    "        # maybe reshape? to (batch_size, seq_len_out, vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_in = torch.randn(batch_size,seq_len_encoder,d_model).to(device).to(dtype)\n",
    "decoder_in = torch.randn(batch_size,seq_len_decoder,d_model).to(device).to(dtype)\n",
    "transformer = Transformer(3,4,seq_len_encoder,seq_len_decoder,64,40000).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()\n",
    "out = transformer(encoder_in,decoder_in)\n",
    "end = time.time()\n",
    "print(f'time: {end - start}')\n",
    "out.shape\n",
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"weights:\",sum(p.numel() for p in transformer.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Marc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0 | loss: 9.823493003845215 time: 0.4517970085144043\n",
      "i:25 | loss: 0.815750002861023 time: 0.18344879150390625\n",
      "i:50 | loss: 0.5972856283187866 time: 0.18168139457702637\n",
      "i:75 | loss: 0.5429232716560364 time: 0.16954612731933594\n",
      "i:100 | loss: 0.5321913361549377 time: 0.2876427173614502\n",
      "epoch: 1\n",
      "i:0 | loss: 0.4799748957157135 time: 0.18187642097473145\n",
      "i:25 | loss: 0.4780837595462799 time: 0.1825394630432129\n",
      "i:50 | loss: 0.4553200602531433 time: 0.17553114891052246\n",
      "i:75 | loss: 0.46833452582359314 time: 0.18348145484924316\n",
      "i:100 | loss: 0.4419066607952118 time: 0.1820204257965088\n",
      "epoch: 2\n",
      "i:0 | loss: 0.43334677815437317 time: 0.18553400039672852\n",
      "i:25 | loss: 0.4146915674209595 time: 0.17900419235229492\n",
      "i:50 | loss: 0.42326411604881287 time: 0.17054486274719238\n",
      "i:75 | loss: 0.3904164135456085 time: 0.18601083755493164\n",
      "i:100 | loss: 0.3928542137145996 time: 0.18249058723449707\n",
      "epoch: 3\n",
      "i:0 | loss: 0.3707960247993469 time: 0.18550419807434082\n",
      "i:25 | loss: 0.3635280132293701 time: 0.18048834800720215\n",
      "i:50 | loss: 0.34039634466171265 time: 0.17752528190612793\n",
      "i:75 | loss: 0.34479063749313354 time: 0.16455960273742676\n",
      "i:100 | loss: 0.33938613533973694 time: 0.16755294799804688\n",
      "epoch: 4\n",
      "i:0 | loss: 0.32330793142318726 time: 0.16356182098388672\n",
      "i:25 | loss: 0.30648842453956604 time: 0.2343745231628418\n",
      "i:50 | loss: 0.3237342834472656 time: 0.18007874488830566\n",
      "i:75 | loss: 0.30215397477149963 time: 0.16356277465820312\n",
      "i:100 | loss: 0.3058373034000397 time: 0.1997208595275879\n",
      "epoch: 5\n",
      "i:0 | loss: 0.2637982666492462 time: 0.19169354438781738\n",
      "i:25 | loss: 0.27594193816185 time: 0.182511568069458\n",
      "i:50 | loss: 0.291338711977005 time: 0.1655566692352295\n",
      "i:75 | loss: 0.26635122299194336 time: 0.18025827407836914\n",
      "i:100 | loss: 0.26808834075927734 time: 0.18131089210510254\n",
      "epoch: 6\n",
      "i:0 | loss: 0.2699843943119049 time: 0.18251347541809082\n",
      "i:25 | loss: 0.23454974591732025 time: 0.18114709854125977\n",
      "i:50 | loss: 0.24524284899234772 time: 0.18066740036010742\n",
      "i:75 | loss: 0.239097461104393 time: 0.1819288730621338\n",
      "i:100 | loss: 0.22687502205371857 time: 0.1805589199066162\n",
      "epoch: 7\n",
      "i:0 | loss: 0.209470734000206 time: 0.18115735054016113\n",
      "i:25 | loss: 0.2138211727142334 time: 0.18053865432739258\n",
      "i:50 | loss: 0.1986587643623352 time: 0.18151569366455078\n",
      "i:75 | loss: 0.21343496441841125 time: 0.19042134284973145\n",
      "i:100 | loss: 0.22205670177936554 time: 0.16579699516296387\n",
      "epoch: 8\n",
      "i:0 | loss: 0.17326971888542175 time: 0.16356229782104492\n",
      "i:25 | loss: 0.18241211771965027 time: 0.16409659385681152\n",
      "i:50 | loss: 0.18584474921226501 time: 0.16256499290466309\n",
      "i:75 | loss: 0.1691427081823349 time: 0.16335415840148926\n",
      "i:100 | loss: 0.16756504774093628 time: 0.16655397415161133\n",
      "epoch: 9\n",
      "i:0 | loss: 0.14106182754039764 time: 0.1635892391204834\n",
      "i:25 | loss: 0.164362832903862 time: 0.16939687728881836\n",
      "i:50 | loss: 0.15526655316352844 time: 0.1625652313232422\n",
      "i:75 | loss: 0.1538923680782318 time: 0.16489720344543457\n",
      "i:100 | loss: 0.1503896415233612 time: 0.16318488121032715\n",
      "epoch: 10\n",
      "i:0 | loss: 0.10178659111261368 time: 0.16080260276794434\n",
      "i:25 | loss: 0.11118831485509872 time: 0.17009592056274414\n",
      "i:50 | loss: 0.11901284754276276 time: 0.191056489944458\n",
      "i:75 | loss: 0.11267136037349701 time: 0.16356277465820312\n",
      "i:100 | loss: 0.10844653844833374 time: 0.16356158256530762\n",
      "epoch: 11\n",
      "i:0 | loss: 0.08006282895803452 time: 0.16755151748657227\n",
      "i:25 | loss: 0.0913284495472908 time: 0.16057062149047852\n",
      "i:50 | loss: 0.08986165374517441 time: 0.17589235305786133\n",
      "i:75 | loss: 0.09740424901247025 time: 0.17741727828979492\n",
      "i:100 | loss: 0.11805557459592819 time: 0.16181206703186035\n",
      "epoch: 12\n",
      "i:0 | loss: 0.05977301672101021 time: 0.16356253623962402\n",
      "i:25 | loss: 0.07402079552412033 time: 0.16455960273742676\n",
      "i:50 | loss: 0.0677189975976944 time: 0.16364097595214844\n",
      "i:75 | loss: 0.0832245796918869 time: 0.16154026985168457\n",
      "i:100 | loss: 0.09164143353700638 time: 0.16309356689453125\n",
      "epoch: 13\n",
      "i:0 | loss: 0.05979518964886665 time: 0.16256403923034668\n",
      "i:25 | loss: 0.05334360897541046 time: 0.17107892036437988\n",
      "i:50 | loss: 0.06086641550064087 time: 0.1816086769104004\n",
      "i:75 | loss: 0.05939881503582001 time: 0.18154501914978027\n",
      "i:100 | loss: 0.058656297624111176 time: 0.18307924270629883\n",
      "epoch: 14\n",
      "i:0 | loss: 0.05283495411276817 time: 0.18151402473449707\n",
      "i:25 | loss: 0.04525918886065483 time: 0.16954493522644043\n",
      "i:50 | loss: 0.041880182921886444 time: 0.18088412284851074\n",
      "i:75 | loss: 0.04973270744085312 time: 0.18395328521728516\n",
      "i:100 | loss: 0.04741805046796799 time: 0.1806659698486328\n",
      "epoch: 15\n",
      "i:0 | loss: 0.047428980469703674 time: 0.17949128150939941\n",
      "i:25 | loss: 0.0336332730948925 time: 0.16156721115112305\n",
      "i:50 | loss: 0.03828563913702965 time: 0.16021418571472168\n",
      "i:75 | loss: 0.04724825918674469 time: 0.16356205940246582\n",
      "i:100 | loss: 0.051619503647089005 time: 0.16356182098388672\n",
      "epoch: 16\n",
      "i:0 | loss: 0.034747496247291565 time: 0.1625375747680664\n",
      "i:25 | loss: 0.03165431693196297 time: 0.165557861328125\n",
      "i:50 | loss: 0.03199275955557823 time: 0.16256499290466309\n",
      "i:75 | loss: 0.0334540531039238 time: 0.16125035285949707\n",
      "i:100 | loss: 0.03246534615755081 time: 0.1655569076538086\n",
      "epoch: 17\n",
      "i:0 | loss: 0.02598719112575054 time: 0.16256427764892578\n",
      "i:25 | loss: 0.02914203144609928 time: 0.17054367065429688\n",
      "i:50 | loss: 0.03836742788553238 time: 0.18307209014892578\n",
      "i:75 | loss: 0.03366832435131073 time: 0.18350911140441895\n",
      "i:100 | loss: 0.029338516294956207 time: 0.18092894554138184\n",
      "epoch: 18\n",
      "i:0 | loss: 0.029328839853405952 time: 0.18297576904296875\n",
      "i:25 | loss: 0.027118386700749397 time: 0.1715412139892578\n",
      "i:50 | loss: 0.025417789816856384 time: 0.18051695823669434\n",
      "i:75 | loss: 0.03081578202545643 time: 0.1815330982208252\n",
      "i:100 | loss: 0.03421225771307945 time: 0.18049097061157227\n",
      "epoch: 19\n",
      "i:0 | loss: 0.023374328389763832 time: 0.182511568069458\n",
      "i:25 | loss: 0.02270669862627983 time: 0.18105077743530273\n",
      "i:50 | loss: 0.023441893979907036 time: 0.1611952781677246\n",
      "i:75 | loss: 0.026542039588093758 time: 0.1825118064880371\n",
      "i:100 | loss: 0.03098616935312748 time: 0.18450689315795898\n",
      "epoch: 20\n",
      "i:0 | loss: 0.023568980395793915 time: 0.18151402473449707\n",
      "i:25 | loss: 0.02585400827229023 time: 0.1838834285736084\n",
      "i:50 | loss: 0.023562822490930557 time: 0.1814873218536377\n",
      "i:75 | loss: 0.03197822347283363 time: 0.1647014617919922\n",
      "i:100 | loss: 0.030445514246821404 time: 0.18051791191101074\n",
      "epoch: 21\n",
      "i:0 | loss: 0.01854695938527584 time: 0.18222498893737793\n",
      "i:25 | loss: 0.025426432490348816 time: 0.18290185928344727\n",
      "i:50 | loss: 0.022189369425177574 time: 0.18450617790222168\n",
      "i:75 | loss: 0.027561599388718605 time: 0.18151354789733887\n",
      "i:100 | loss: 0.022984787821769714 time: 0.16356253623962402\n",
      "epoch: 22\n",
      "i:0 | loss: 0.018617210909724236 time: 0.16758155822753906\n",
      "i:25 | loss: 0.01875443570315838 time: 0.18051695823669434\n",
      "i:50 | loss: 0.0229816772043705 time: 0.16356253623962402\n",
      "i:75 | loss: 0.02365441620349884 time: 0.18350887298583984\n",
      "i:100 | loss: 0.01951487921178341 time: 0.18949246406555176\n",
      "epoch: 23\n",
      "i:0 | loss: 0.018237682059407234 time: 0.16655302047729492\n",
      "i:25 | loss: 0.016144486144185066 time: 0.1825113296508789\n",
      "i:50 | loss: 0.02433040738105774 time: 0.18102431297302246\n",
      "i:75 | loss: 0.024229547008872032 time: 0.16456007957458496\n",
      "i:100 | loss: 0.024017611518502235 time: 0.18110895156860352\n",
      "epoch: 24\n",
      "i:0 | loss: 0.01449256669729948 time: 0.1825113296508789\n",
      "i:25 | loss: 0.014650079421699047 time: 0.18051671981811523\n",
      "i:50 | loss: 0.015315275639295578 time: 0.18220114707946777\n",
      "i:75 | loss: 0.020163582637906075 time: 0.18152976036071777\n",
      "i:100 | loss: 0.01679205894470215 time: 0.1685495376586914\n"
     ]
    }
   ],
   "source": [
    "path = \"D:\\Transformer\\\\\"\n",
    "vocab_in = joblib.load(path+f'vocab_{language_in}.data')\n",
    "vocab_out = joblib.load(path+f'vocab_{language_out}.data')\n",
    "\n",
    "num_cells = 3\n",
    "cell_embedding_size = 64\n",
    "num_heads = 4\n",
    "seq_len_encoder = vocab_in[\"max_sentence_len\"] + 1\n",
    "seq_len_decoder = vocab_out[\"max_sentence_len\"] + 1\n",
    "vocab_size = vocab_out[\"vocab_size\"] + 1 # + <EOS>\n",
    "\n",
    "transformer = Transformer(num_cells,num_heads,\n",
    "                          seq_len_encoder,seq_len_decoder,\n",
    "                          cell_embedding_size,vocab_size).to(device)\n",
    "\n",
    "dataset = Language_DataSet(path, language_in, language_out)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=0, shuffle=True,drop_last=True)\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "lr = 5e-4\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(25):\n",
    "    print(\"epoch:\",epoch)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # data\n",
    "        x_in, x_out, target = batch\n",
    "        \n",
    "        # reshape target for cross entropy loss\n",
    "        target = torch.flatten(target)\n",
    "        \n",
    "        # run transformer\n",
    "        out = transformer(x_in, x_out)\n",
    "        #print(out.shape)\n",
    "        #print(target.shape)\n",
    "        #print(torch.flatten(target).shape)\n",
    "        \n",
    "        loss = cross_entropy_loss(out, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        if i%25 == 0:\n",
    "            print(f'i:{i} | loss: {loss} time: {end - start}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show token predictions while training \n",
    "(not inference atm and only small dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\Marc\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where 's Boston ? \n",
      "Tom is a psycho . \n",
      "Happy birthday ! \n",
      "Follow me . \n",
      "Do it your way . \n",
      "Why do you work ? \n",
      "They kissed . \n",
      "I 'm biased . \n",
      "Stay calm . \n",
      "I 'm so fat . \n",
      "Tom has changed . \n",
      "Tom felt tired . \n",
      "Tom teaches . \n",
      "I love jokes . \n",
      "Tom drinks beer . \n",
      "Do n't hurt him . \n",
      "Examine this . \n",
      "We need them . \n",
      "You were brave . \n",
      "Is dinner ready ? \n",
      "I do n't want it . \n",
      "Stay for supper . \n",
      "Who succeeded ? \n",
      "Tom is credible . \n",
      "I know Tom . \n",
      "Who are they ? \n",
      "We respect them . \n",
      "Just swim . \n",
      "Tom enlisted . \n",
      "Open the doors . \n",
      "It is n't new . \n",
      "They saw me . \n",
      "Tom loves pasta . \n",
      "We 're very poor . \n",
      "We 're rich . \n",
      "I felt left out . \n",
      "When do you going ? \n",
      "How about you ? \n",
      "Come back in . \n",
      "I was at home . \n",
      "Tom felt cold . \n",
      "Kill them . \n",
      "How do I I it ? \n",
      "Tom is glad . \n",
      "Go lost . \n",
      "Breathe deeply . \n",
      "I 'll take Tom . \n",
      "He ca n't do it . \n",
      "I ca n't do it . \n",
      "I 'll pay . \n"
     ]
    }
   ],
   "source": [
    "for j in range(50):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # data\n",
    "        x_in, x_out, _ = batch\n",
    "\n",
    "        # run transformer\n",
    "        out = transformer(x_in, x_out)\n",
    "        #print(out.shape)\n",
    "        # reshape to batch_size, seq, vocab\n",
    "        out = torch.reshape(out, (batch_size, seq_len_decoder,vocab_size))\n",
    "        #print(out.shape)\n",
    "        out = F.softmax(out, dim=2) \n",
    "        s = out[0]\n",
    "        max_index = torch.argmax(s, dim=1).cpu().numpy()\n",
    "        #print(max_index)\n",
    "\n",
    "        s = \"\"\n",
    "        for index in max_index:\n",
    "            token = vocab_out[index]\n",
    "            if token != \"<EOS>\":\n",
    "                s +=  token+\" \"\n",
    "        print(s)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(transformer,f\"transformer.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
