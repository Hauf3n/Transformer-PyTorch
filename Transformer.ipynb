{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from flair.data import Sentence\n",
    "from flair.data import Token\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "\n",
    "d_model = 300 # embedding size of fasttext models\n",
    "batch_size = 128\n",
    "seq_len_encoder = 32\n",
    "seq_len_decoder = 36\n",
    "output_lang = 'en' # select language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select translation scheme in -> out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_out = None\n",
    "\n",
    "if output_lang == 'en'\n",
    "    embedding_out = WordEmbeddings('en')\n",
    "else:\n",
    "    embedding_out = WordEmbeddings('de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_DataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):#D:\\Transformer\\dataset.data\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = joblib.load(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # return format: input_encoding, labels\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        print(sample[1].shape)\n",
    "        \n",
    "        item = None\n",
    "        if output_lang == 'en':\n",
    "            item = (sample[1],sample[2])\n",
    "        else:\n",
    "            item = (sample[0],sample[3])\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = Language_DataSet('D:\\Transformer\\dataset.data')\n",
    "#dl = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "#for eps_data in dl:\n",
    "    #print(i)\n",
    "    #print(eps_data[1])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Implementation\n",
    "## Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, seq_len, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dimensions = dimensions\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, dimensions * heads * 3)\n",
    "        self.final_linear = nn.Linear(self.heads * self.dimensions, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # reshape for linear qkv layer\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # compute q,v,k for every head\n",
    "        qkv = self.qkv(x) # (seq * batch, heads * 3 * dimensions)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * 3 * self.dimensions))\n",
    "        # split into heads and seperate q, k, v in different dims\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads, 3, self.dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        qkv = qkv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # extract q, k, v\n",
    "        q = qkv[:,:,:,0,:]\n",
    "        k = qkv[:,:,:,1,:]\n",
    "        v = qkv[:,:,:,2,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        k = torch.reshape(k, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.dimensions).to(device).to(dtype))\n",
    "        # optional masking\n",
    "        if mask is not None:\n",
    "            qk[mask == 1] = float('-inf')\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)     \n",
    "        \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, self.seq_len, self.dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * self.dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * self.seq_len, self.heads * self.dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, seq_len, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, seq_len, attention_dimension)\n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z = self.self_attention(x)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_1, (batch_size*self.seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + ff_out\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_1(residual_2)\n",
    "        \n",
    "        return norm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len, attention_dimensions):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        \n",
    "        # stacked encoder cells\n",
    "        encoder_cells = [ Encoder_Cell(heads, seq_len, attention_dimensions).to(device) for i in range(cells)]\n",
    "        self.encode = nn.Sequential(*encoder_cells)\n",
    "        \n",
    "        # key and value output of encoder\n",
    "        self.kv = nn.Linear(d_model, attention_dimensions * heads * 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encoding shape: batch_size, seq_len, d_model\n",
    "        encoding = self.encode(x) \n",
    "        \n",
    "        # reshape to feed into linear kv layer\n",
    "        encoding = torch.reshape(encoding, (batch_size * self.seq_len, d_model))\n",
    "        \n",
    "        # apply linear\n",
    "        kv = self.kv(encoding)\n",
    "        # reshape back\n",
    "        kv = torch.reshape(kv, (batch_size, self.seq_len, self.attention_dimensions * self.heads * 2))\n",
    "        \n",
    "        # seperate k and v\n",
    "        kv = torch.reshape(kv, (batch_size, self.seq_len, self.heads, 2, self.attention_dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        kv = kv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # split k, v\n",
    "        k = kv[:,:,:,0,:]\n",
    "        v = kv[:,:,:,1,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        k = torch.reshape(k, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        \n",
    "        return k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, seq_len, attention_dimensions):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        \n",
    "        self.q = nn.Linear(d_model, attention_dimensions * heads).to(device)\n",
    "        self.final_linear = nn.Linear(heads * attention_dimensions, d_model).to(device)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch_size, seq_len, d_model\n",
    "        # encoder k/v shape: batch_size*heads, seq_len, attention_dimensions\n",
    "        \n",
    "        # reshape for linear q layer\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # compute q for every head\n",
    "        q = self.q(x) # (seq * batch, heads * attention_dimensions)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        q = torch.reshape(q, (batch_size, self.seq_len, self.heads * self.attention_dimensions))\n",
    "        # split into heads \n",
    "        q = torch.reshape(q, (batch_size, self.seq_len, self.heads, self.attention_dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        q = q.permute(0,2,1,3)\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(encoder_k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.attention_dimensions).to(device).to(dtype))\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)     \n",
    "        \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, encoder_v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, self.seq_len, self.attention_dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * self.attention_dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * self.seq_len, self.heads * self.attention_dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        return z     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Cell(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, seq_len, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # construct decoder mask # MASK CORRECT?????\n",
    "        a = np.triu(np.ones((seq_len,seq_len)), k=1) \n",
    "        mask = torch.unsqueeze(torch.tensor(a).to(device).long(),dim=0)\n",
    "        self.mask = mask.repeat(batch_size*heads,1,1)\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, seq_len, attention_dimension).to(device)\n",
    "        self.enc_dec_attention = Encoder_Decoder_Attention(heads, seq_len, attention_dimension).to(device)\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_3 = nn.LayerNorm([d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z_1 = self.self_attention(x, self.mask)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z_1\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        z_2 = self.enc_dec_attention(norm_1, encoder_k, encoder_v)\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + z_2\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_2(residual_2)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_2, (batch_size*self.seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        # 3rd residual\n",
    "        residual_3 = norm_2 + ff_out\n",
    "        # 3rd norm\n",
    "        norm_3 = self.layer_norm_3(residual_3)\n",
    "        \n",
    "        return norm_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len, attention_dimensions, vocab_size):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # stacked encoder cells\n",
    "        self.decoder_cells = [ Decoder_Cell(heads, seq_len, attention_dimensions).to(device) for i in range(cells)]\n",
    "        \n",
    "        # output layer and then softmax\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        \n",
    "        for decoder_cell in self.decoder_cells:\n",
    "            x = decoder_cell(x,encoder_k, encoder_v)\n",
    "            \n",
    "        # reshape for linear\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # feed in laysr\n",
    "        x = self.final_linear(x)\n",
    "        \n",
    "        # softmax over vocab_size\n",
    "        softmax = F.softmax(x, dim=1)\n",
    "        \n",
    "        return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len_enc, seq_len_dec, attention_dimensions, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(cells, heads, seq_len_enc, attention_dimensions).to(device)\n",
    "        self.decoder = Decoder(cells, heads, seq_len_dec, attention_dimensions, vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x_encoder, x_decoder):\n",
    "        # x_in shape: batch_size, seq_len_in, d_model\n",
    "        # x_out shape: batch_size, seq_len_out, d_model\n",
    "        \n",
    "        encoder_k, encoder_v = self.encoder(x_encoder)\n",
    "        out = self.decoder(x_decoder, encoder_k, encoder_v)\n",
    "        \n",
    "        # maybe reshape? to (batch_size, seq_len_out, vocab_size)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder_in = torch.randn(batch_size,seq_len,d_model).to(device).to(dtype)\n",
    "decoder = Decoder_Cell(4,64).to(device)\n",
    "out = decoder(decoder_in)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_in = torch.randn(batch_size,seq_len_encoder,d_model).to(device).to(dtype)\n",
    "decoder_in = torch.randn(batch_size,seq_len_decoder,d_model).to(device).to(dtype)\n",
    "\n",
    "transformer = Transformer(3,4,seq_len_encoder,seq_len_decoder,64,40000).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0.03393268585205078\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "out = transformer(encoder_in,decoder_in)\n",
    "end = time.time()\n",
    "print(f'time: {end - start}')\n",
    "out.shape\n",
    "del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 14969688\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\",sum(p.numel() for p in transformer.parameters() if p.requires_grad))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
