{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import time\n",
    "from flair.data import Sentence\n",
    "from flair.data import Token\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "\n",
    "# embedding size of fasttext models\n",
    "d_model = 300 \n",
    "\n",
    "# select language\n",
    "language_in = 'de'\n",
    "language_out = 'en' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_DataSet(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, path, language_in, language_out):#D:\\Transformer\\dataset.data\n",
    "        super().__init__()\n",
    "        self.language_in = language_in\n",
    "        self.language_out = language_out\n",
    "\n",
    "        # load dat and vocab\n",
    "        self.data = joblib.load(path+f'{language_in}_to_{language_out}.data')\n",
    "        self.vocab_in = joblib.load(path+f'vocab_{language_in}.data')\n",
    "        self.vocab_out = joblib.load(path+f'vocab_{language_out}.data')\n",
    "\n",
    "        # zero padding info\n",
    "        self.seq_len_in = self.vocab_in[\"max_sentence_len\"] + 1 # additional <SOS>/<EOS> token\n",
    "        self.seq_len_out = self.vocab_out[\"max_sentence_len\"] + 1 # additional <SOS>/<EOS> token\n",
    "\n",
    "        # precompute padding\n",
    "        self.precompute_padding = torch.zeros(1, d_model).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return format: input_encoding, output_encoding, target\n",
    "\n",
    "        x_in, x_out, target = self.data[index]\n",
    "        x_in = x_in.to(device)\n",
    "        x_out = x_out.to(device)\n",
    "        \n",
    "        # zero padding\n",
    "\n",
    "        seq_len_in = x_in.shape[0]\n",
    "        padding_length_in = self.seq_len_in - seq_len_in\n",
    "\n",
    "        seq_len_out = x_out.shape[0]\n",
    "        padding_length_out = self.seq_len_out - seq_len_out\n",
    "\n",
    "        padding_in = self.precompute_padding.repeat([padding_length_in,1])\n",
    "        padding_out = self.precompute_padding.repeat([padding_length_out,1])\n",
    "\n",
    "        x_in = torch.cat([x_in, padding_in], dim=0)\n",
    "        x_out = torch.cat([x_out, padding_out], dim=0)\n",
    "\n",
    "        # padding target <EOS> as target for padding?\n",
    "        target_len = len(target)\n",
    "        padding_length = self.seq_len_out - target_len\n",
    "        padding_index = target[-1] # <EOS>\n",
    "\n",
    "        target = target + [ padding_index for i in range(padding_length)]\n",
    "        target = torch.tensor(target).to(device)\n",
    "\n",
    "        return x_in, x_out, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path = \"D:\\Transformer\\\\\"\n",
    "dataset = Language_DataSet(path, language_in, language_out)\n",
    "dl = DataLoader(dataset, batch_size=32,num_workers=4, shuffle=True)\n",
    "if __name__ == '__main__':\n",
    "    for eps_data in dl:\n",
    "        x_in, x_out, target = eps_data\n",
    "        print(x_in)\n",
    "        print(x_out)\n",
    "        print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Implementation\n",
    "## Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, seq_len, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dimensions = dimensions\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, self.dimensions * heads * 3)\n",
    "        self.final_linear = nn.Linear(self.heads * self.dimensions, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # reshape for linear qkv layer\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # compute q,v,k for every head\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * 3 * self.dimensions))\n",
    "        # split into heads and seperate q, k, v in different dims\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads, 3, self.dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        qkv = qkv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # extract q, k, v\n",
    "        q = qkv[:,:,:,0,:]\n",
    "        k = qkv[:,:,:,1,:]\n",
    "        v = qkv[:,:,:,2,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        k = torch.reshape(k, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, self.seq_len, self.dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.dimensions).to(device).to(dtype))\n",
    "        # optional masking\n",
    "        if mask is not None:\n",
    "            qk[mask == 1] = float('-inf')\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)\n",
    "        \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, self.seq_len, self.dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * self.dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * self.seq_len, self.heads * self.dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, seq_len, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, seq_len, attention_dimension)\n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z = self.self_attention(x)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_1, (batch_size*self.seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + ff_out\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_1(residual_2)\n",
    "        \n",
    "        return norm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len, attention_dimensions):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        \n",
    "        # stacked encoder cells\n",
    "        encoder_cells = [ Encoder_Cell(heads, seq_len, attention_dimensions).to(device) for i in range(cells)]\n",
    "        self.encode = nn.Sequential(*encoder_cells)\n",
    "        \n",
    "        # key and value output of encoder\n",
    "        self.kv = nn.Linear(d_model, attention_dimensions * heads * 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # encoding shape: batch_size, seq_len, d_model\n",
    "        encoding = self.encode(x) \n",
    "        \n",
    "        # reshape to feed into linear kv layer\n",
    "        encoding = torch.reshape(encoding, (batch_size * self.seq_len, d_model))\n",
    "        \n",
    "        # apply linear\n",
    "        kv = self.kv(encoding)\n",
    "        # reshape back\n",
    "        kv = torch.reshape(kv, (batch_size, self.seq_len, self.attention_dimensions * self.heads * 2))\n",
    "        \n",
    "        # seperate k and v\n",
    "        kv = torch.reshape(kv, (batch_size, self.seq_len, self.heads, 2, self.attention_dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        kv = kv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # split k, v\n",
    "        k = kv[:,:,:,0,:]\n",
    "        v = kv[:,:,:,1,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        k = torch.reshape(k, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        \n",
    "        return k, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, seq_len, attention_dimensions):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        \n",
    "        self.q = nn.Linear(d_model, attention_dimensions * heads).to(device)\n",
    "        self.final_linear = nn.Linear(heads * attention_dimensions, d_model).to(device)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch_size, seq_len, d_model\n",
    "        # encoder k/v shape: batch_size*heads, seq_len, attention_dimensions\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # reshape for linear q layer\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # compute q for every head\n",
    "        q = self.q(x) # (seq * batch, heads * attention_dimensions)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        q = torch.reshape(q, (batch_size, self.seq_len, self.heads * self.attention_dimensions))\n",
    "        # split into heads \n",
    "        q = torch.reshape(q, (batch_size, self.seq_len, self.heads, self.attention_dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        q = q.permute(0,2,1,3)\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, self.seq_len, self.attention_dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(encoder_k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.attention_dimensions).to(device).to(dtype))\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)     \n",
    "        \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, encoder_v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, self.seq_len, self.attention_dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.seq_len, self.heads * self.attention_dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * self.seq_len, self.heads * self.attention_dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        return z     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Cell(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, seq_len, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.heads = heads\n",
    "        \n",
    "        # construct decoder mask\n",
    "        a = np.triu(np.ones((seq_len,seq_len)), k=1) \n",
    "        self.mask = torch.unsqueeze(torch.tensor(a).to(device).long(),dim=0)\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, seq_len, attention_dimension).to(device)\n",
    "        self.enc_dec_attention = Encoder_Decoder_Attention(heads, seq_len, attention_dimension).to(device)\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_3 = nn.LayerNorm([d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        mask = self.mask.repeat(batch_size*self.heads,1,1)\n",
    "        z_1 = self.self_attention(x, mask)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z_1\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        z_2 = self.enc_dec_attention(norm_1, encoder_k, encoder_v)\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + z_2\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_2(residual_2)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_2, (batch_size*self.seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, self.seq_len, d_model))\n",
    "        \n",
    "        # 3rd residual\n",
    "        residual_3 = norm_2 + ff_out\n",
    "        # 3rd norm\n",
    "        norm_3 = self.layer_norm_3(residual_3)\n",
    "        \n",
    "        return norm_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len, attention_dimensions, vocab_size):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.seq_len = seq_len\n",
    "        self.attention_dimensions = attention_dimensions\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # stacked encoder cells\n",
    "        self.decoder_cells = [ Decoder_Cell(heads, seq_len, attention_dimensions).to(device) for i in range(cells)]\n",
    "        \n",
    "        # output layer and then softmax\n",
    "        self.final_linear = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        for decoder_cell in self.decoder_cells:\n",
    "            x = decoder_cell(x,encoder_k, encoder_v)\n",
    "            \n",
    "        # reshape for linear\n",
    "        x = torch.reshape(x, (batch_size*self.seq_len, d_model))\n",
    "        \n",
    "        # feed in final layer\n",
    "        x = self.final_linear(x)\n",
    "          \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, seq_len_enc, seq_len_dec, attention_dimensions, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = Encoder(cells, heads, seq_len_enc, attention_dimensions).to(device)\n",
    "        self.decoder = Decoder(cells, heads, seq_len_dec, attention_dimensions, vocab_size).to(device)\n",
    "    \n",
    "    def forward(self, x_encoder, x_decoder):\n",
    "        # x_in shape: batch_size, seq_len_in, d_model\n",
    "        # x_out shape: batch_size, seq_len_out, d_model\n",
    "        \n",
    "        encoder_k, encoder_v = self.encoder(x_encoder)\n",
    "        out = self.decoder(x_decoder, encoder_k, encoder_v)\n",
    "        \n",
    "        # output needs softmax afterwards | cross_entropy_loss or F.softmax\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:\\Transformer\\\\\"\n",
    "vocab_in = joblib.load(path+f'vocab_{language_in}.data')\n",
    "vocab_out = joblib.load(path+f'vocab_{language_out}.data')\n",
    "\n",
    "batch_size_dl = 128\n",
    "epochs = 20\n",
    "lr = 5e-4\n",
    "num_cells = 4\n",
    "cell_embedding_size = 64\n",
    "num_heads = 6\n",
    "\n",
    "seq_len_encoder = vocab_in[\"max_sentence_len\"] + 1\n",
    "seq_len_decoder = vocab_out[\"max_sentence_len\"] + 1\n",
    "vocab_size = vocab_out[\"vocab_size\"] + 1 # + <EOS>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_cells,num_heads,\n",
    "                          seq_len_encoder,seq_len_decoder,\n",
    "                          cell_embedding_size,vocab_size).to(device)\n",
    "\n",
    "print(\"model weights:\",sum(p.numel() for p in transformer.parameters() if p.requires_grad))\n",
    "\n",
    "dataset = Language_DataSet(path, language_in, language_out)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size_dl, num_workers=0, shuffle=True,drop_last=True)\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"epoch:\",epoch)\n",
    "    \n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        start = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # data\n",
    "        x_in, x_out, target = batch\n",
    "        \n",
    "        # reshape target for cross entropy loss\n",
    "        target = torch.flatten(target)\n",
    "        \n",
    "        # run transformer\n",
    "        out = transformer(x_in, x_out)\n",
    "        \n",
    "        loss = cross_entropy_loss(out, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        end = time.time()\n",
    "        \n",
    "        if i%25 == 0:\n",
    "            print(f'i:{i} | loss: {loss} time: {end - start}')\n",
    "        \n",
    "    epoch_end = time.time()\n",
    "    print(f'epoch time: {epoch_end - epoch_start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.save(transformer,f\"transformer.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# do inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_mask = [2*i for i in range(d_model//2)]\n",
    "cosine_mask = [2*i+1 for i in range(d_model//2)]\n",
    "i = torch.tensor([i for i in range(d_model//2)]).to(device).to(dtype)\n",
    "\n",
    "# pre compute positional encodings for 50 tokens\n",
    "positional_encodings = []\n",
    "for position in range(50):\n",
    "    \n",
    "    sine = torch.sin( position / ( 10000 ** (2*i/d_model) ) )\n",
    "    cosine = torch.cos( position / ( 10000 ** (2*i/d_model) ) )\n",
    "    \n",
    "    position_enc = torch.zeros(d_model).to(device)\n",
    "    position_enc[sine_mask] = sine\n",
    "    position_enc[cosine_mask] = cosine\n",
    "\n",
    "    positional_encodings.append(position_enc)\n",
    "\n",
    "# return pre computed pos encoding\n",
    "def positional_encoding(sentence):\n",
    "    num_tokens = len(sentence)\n",
    "    return positional_encodings[0:num_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embedding models\n",
    "embedding_model_en = WordEmbeddings('en')\n",
    "embedding_model_de = WordEmbeddings('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "# constructed sentence\n",
    "sentence_translated = \"\"\n",
    "\n",
    "# test sentence\n",
    "#german = 'Wie alt bist du?'\n",
    "#german = 'Er ist Rechtsanwalt.'\n",
    "#german = 'Ich bin sehr begeistert.'\n",
    "#german = 'Das ist ein Problem.'\n",
    "#german = 'Ich mag das.'\n",
    "german = 'Wie viele Menschen gibt es?'\n",
    "\n",
    "sentence_de = Sentence(german,language_code='de')\n",
    "# add <EOS> token\n",
    "sentence_de.add_token(\"<EOS>\")\n",
    "# embed\n",
    "embedding_model_de.embed(sentence_de)\n",
    "# pos encoding\n",
    "pos_enc = positional_encoding(sentence_de)\n",
    "# add embedding and pos enc\n",
    "emb_in = torch.stack([token.embedding + pos_enc[i] for i,token in enumerate(sentence_de)])\n",
    "# add padding\n",
    "seq_len_in = emb_in.shape[0]\n",
    "padding_length_in = vocab_in[\"max_sentence_len\"] + 1 - seq_len_in            \n",
    "padding = torch.zeros(padding_length_in, d_model).to(device)\n",
    "                     \n",
    "x_in = torch.unsqueeze(torch.cat([emb_in, padding], dim=0),dim=0)\n",
    "\n",
    "# starting point of ouput\n",
    "sentence_en = Sentence('')\n",
    "sentence_en.add_token('<SOS>')\n",
    "\n",
    "done = False\n",
    "i= 0\n",
    "while not done and i < vocab_out[\"max_sentence_len\"] + 1:\n",
    "    #print(sentence_en)\n",
    "    \n",
    "    # embed\n",
    "    embedding_model_en.embed(sentence_en)\n",
    "    # pos encoding\n",
    "    pos_enc = positional_encoding(sentence_en)\n",
    "    # add embedding and pos enc\n",
    "    emb_out = torch.stack([token.embedding + pos_enc[i] for i,token in enumerate(sentence_en)])\n",
    "    # add padding\n",
    "    seq_len_out = emb_out.shape[0]\n",
    "    padding_length_out = vocab_out[\"max_sentence_len\"] + 1 - seq_len_out            \n",
    "    padding = torch.zeros(padding_length_out, d_model).to(device)\n",
    "    x_out = torch.unsqueeze(torch.cat([emb_out, padding], dim=0),dim=0)\n",
    "    out = transformer(x_in, x_out)\n",
    "    \n",
    "    # reshape to batch_size, seq, vocab\n",
    "    out = torch.reshape(out, (1, seq_len_decoder,vocab_size))\n",
    "    out = F.softmax(out, dim=2) \n",
    "    max_index = torch.argmax(out[0], dim=1).cpu().numpy()\n",
    "    # get relevant position\n",
    "    token_index = max_index[i]\n",
    "    \n",
    "    token = vocab_out[token_index]\n",
    "    if token == \"<EOS>\":\n",
    "        done = True\n",
    "    else:\n",
    "        sentence_translated += token + \" \"\n",
    "        print(\"NEW TOKEN >\",token,\"<\")\n",
    "        sentence_en.add_token(token)\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "print(f'Input: {german} | Translation: {sentence_translated}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
