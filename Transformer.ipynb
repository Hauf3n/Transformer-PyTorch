{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import joblib\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from flair.data import Sentence\n",
    "from flair.embeddings import WordEmbeddings\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "\n",
    "d_model = 300 # embedding size of fasttext models\n",
    "batch_size = 64\n",
    "seq_len = 32\n",
    "output_lang = 'en' # select language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select translation scheme in -> out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_out = None\n",
    "\n",
    "if output_lang == 'en'\n",
    "    embedding_out = WordEmbeddings('en')\n",
    "else:\n",
    "    embedding_out = WordEmbeddings('de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language_DataSet(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path):#D:\\Transformer\\dataset.data\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = joblib.load(path)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # return format: input_encoding, labels\n",
    "        \n",
    "        sample = self.data[index]\n",
    "        print(sample[1].shape)\n",
    "        \n",
    "        item = None\n",
    "        if output_lang == 'en':\n",
    "            item = (sample[1],sample[2])\n",
    "        else:\n",
    "            item = (sample[0],sample[3])\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = Language_DataSet('D:\\Transformer\\dataset.data')\n",
    "#dl = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "#for eps_data in dl:\n",
    "    #print(i)\n",
    "    #print(eps_data[1])\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Implementation\n",
    "## Multi-Head-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dimensions = dimensions\n",
    "        \n",
    "        self.qkv = nn.Linear(d_model, dimensions * heads * 3)\n",
    "        self.final_linear = nn.Linear(self.heads * self.dimensions, d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # reshape for linear qkv layer\n",
    "        x = torch.reshape(x, (batch_size*seq_len, d_model))\n",
    "        \n",
    "        # compute q,v,k for every head\n",
    "        qkv = self.qkv(x) # (seq * batch, heads * 3 * dimensions)\n",
    "\n",
    "        # reshape into (batch_size,seq_len,...)\n",
    "        qkv = torch.reshape(qkv, (batch_size, seq_len, self.heads * 3 * self.dimensions))\n",
    "        # split into heads and seperate q, k, v in different dims\n",
    "        qkv = torch.reshape(qkv, (batch_size, seq_len, self.heads, 3, self.dimensions))\n",
    "        \n",
    "        # permute head to front for parallel processing\n",
    "        qkv = qkv.permute(0,2,1,3,4)\n",
    "        \n",
    "        # extract q, k, v\n",
    "        q = qkv[:,:,:,0,:]\n",
    "        k = qkv[:,:,:,1,:]\n",
    "        v = qkv[:,:,:,2,:]\n",
    "        \n",
    "        # fuse batch_size and head dim for parallel processing\n",
    "        q = torch.reshape(q, (batch_size * self.heads, seq_len, self.dimensions))\n",
    "        k = torch.reshape(k, (batch_size * self.heads, seq_len, self.dimensions))\n",
    "        v = torch.reshape(v, (batch_size * self.heads, seq_len, self.dimensions))\n",
    "        \n",
    "        # transpose k\n",
    "        k = torch.transpose(k, 1, 2)\n",
    "        \n",
    "        # multiply q and k\n",
    "        qk = torch.bmm(q,k)\n",
    "        # scale\n",
    "        qk = qk / torch.sqrt(torch.tensor(self.dimensions).to(device).to(dtype))\n",
    "        # optional masking\n",
    "        if mask is not None:\n",
    "            qk[mask == 1] = float('-inf')\n",
    "        # softmax\n",
    "        qk = F.softmax(qk, dim=2)\n",
    "        return qk        \n",
    "    \n",
    "        # multiply with v\n",
    "        qkv = torch.bmm(qk, v)\n",
    "        \n",
    "        # reshape to cat heads\n",
    "        qkv = torch.reshape(qkv, (batch_size, self.heads, seq_len, self.dimensions))\n",
    "        # cat all heads\n",
    "        qkv = qkv.permute(0,2,1,3)\n",
    "        qkv = torch.reshape(qkv, (batch_size, seq_len, self.heads * self.dimensions))\n",
    "        \n",
    "        # reshape to multiply with final linear\n",
    "        qkv = torch.reshape(qkv, (batch_size * seq_len, self.heads * self.dimensions))\n",
    "        # multiply with final linear\n",
    "        z = self.final_linear(qkv)\n",
    "        \n",
    "        # reshape to input format\n",
    "        z = torch.reshape(z, (batch_size, seq_len, d_model))\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, attention_dimension, ff_inner=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, attention_dimension)\n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z = self.self_attention(x)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_1, (batch_size*seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, seq_len, d_model))\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + ff_out\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_1(residual_2)\n",
    "        \n",
    "        return norm_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, cells, heads, attention_dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        encoder_cells = [ Encoder_Cell(heads, attention_dimensions) for i in range(cells)]\n",
    "        self.encode = nn.Sequential(*encoder_cells)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoding = self.encode(x)\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Decoder_Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, attention_dimension):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.q = nn.Linear(d_model, dimensions * heads)\n",
    "        self.final_linear = nn.Linear(self.heads * self.dimensions, d_model) \n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_Cell(nn.Module):\n",
    "\n",
    "    def __init__(self, heads, attention_dimension, , ff_inner=1024):\n",
    "        super().__init__()\n",
    "        \n",
    "        # construct decoder mask # MASK CORRECT?????\n",
    "        a = np.triu(np.ones((seq_len,seq_len)), k=1) \n",
    "        mask = torch.unsqueeze(torch.tensor(a).to(device).long(),dim=0)\n",
    "        self.mask = mask.repeat(batch_size*heads,1,1)\n",
    "        \n",
    "        self.self_attention = Multi_Head_Attention(heads, attention_dimension)\n",
    "        self.enc_dec_attention = Encoder_Decoder_Attention(heads, attention_dimension)\n",
    "        \n",
    "        self.layer_norm_1 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_2 = nn.LayerNorm([d_model])\n",
    "        self.layer_norm_3 = nn.LayerNorm([d_model])\n",
    "        \n",
    "        ff_network = [\n",
    "            nn.Linear(d_model, ff_inner),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_inner, d_model),\n",
    "        ]\n",
    "        self.feed_forward_net = nn.Sequential(*ff_network)\n",
    "        \n",
    "    def forward(self, x, encoder_k, encoder_v):\n",
    "        # x shape: batch,seq_len,d_model\n",
    "        batch_size, _, _ = x.shape\n",
    "        \n",
    "        # self attention\n",
    "        z_1 = self.self_attention(x)\n",
    "        \n",
    "        # 1st residual\n",
    "        residual_1 = x + z_1\n",
    "        # 1st norm\n",
    "        norm_1 = self.layer_norm_1(residual_1)\n",
    "        \n",
    "        # encoder-decoder attention\n",
    "        z_2 = self.enc_dec_attention(norm_1, encoder_k, encoder_v)\n",
    "        \n",
    "        # 2nd residual\n",
    "        residual_2 = norm_1 + z_2\n",
    "        # 2nd norm\n",
    "        norm_2 = self.layer_norm_2(residual_2)\n",
    "        \n",
    "        # reshape norm for feed forward network\n",
    "        ff_in = torch.reshape(norm_2, (batch_size*seq_len, d_model))\n",
    "        # feed forward\n",
    "        ff_out = self.feed_forward_net(ff_in)\n",
    "        # reshape back\n",
    "        ff_out = torch.reshape(ff_out, (batch_size, seq_len, d_model))\n",
    "        \n",
    "        # 3rd residual\n",
    "        residual_3 = norm_2 + ff_out\n",
    "        # 3rd norm\n",
    "        norm_3 = self.layer_norm_3(residual_3)\n",
    "        \n",
    "        return norm_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_in = torch.randn(batch_size,seq_len,d_model).to(device).to(dtype)\n",
    "decoder = Decoder_Cell(4,64).to(device)\n",
    "out = decoder(decoder_in)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5607, 0.4393, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3693, 0.2933, 0.3375,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0496, 0.0253, 0.0292,  ..., 0.0292, 0.0000, 0.0000],\n",
       "         [0.0625, 0.0624, 0.0211,  ..., 0.0262, 0.0349, 0.0000],\n",
       "         [0.0228, 0.0234, 0.0183,  ..., 0.0464, 0.0207, 0.0172]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.6350, 0.3650, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3662, 0.2677, 0.3661,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0316, 0.0240, 0.0366,  ..., 0.0524, 0.0000, 0.0000],\n",
       "         [0.0265, 0.0173, 0.0492,  ..., 0.0335, 0.0365, 0.0000],\n",
       "         [0.0173, 0.0188, 0.0330,  ..., 0.0229, 0.0258, 0.0283]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3467, 0.6533, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4725, 0.2850, 0.2426,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0694, 0.0367, 0.0494,  ..., 0.0338, 0.0000, 0.0000],\n",
       "         [0.0392, 0.0318, 0.0343,  ..., 0.0367, 0.0299, 0.0000],\n",
       "         [0.0506, 0.0509, 0.0523,  ..., 0.0413, 0.0194, 0.0276]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5674, 0.4326, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3192, 0.2776, 0.4032,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0289, 0.0412, 0.0696,  ..., 0.0356, 0.0000, 0.0000],\n",
       "         [0.0239, 0.0240, 0.0190,  ..., 0.0295, 0.0482, 0.0000],\n",
       "         [0.0216, 0.0188, 0.0284,  ..., 0.0222, 0.0362, 0.0230]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.2836, 0.7164, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3945, 0.4213, 0.1842,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0312, 0.0434, 0.0361,  ..., 0.0233, 0.0000, 0.0000],\n",
       "         [0.0383, 0.0293, 0.0234,  ..., 0.0312, 0.0333, 0.0000],\n",
       "         [0.0333, 0.0197, 0.0134,  ..., 0.0447, 0.0255, 0.0157]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4450, 0.5550, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.4551, 0.2005, 0.3444,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.0223, 0.0413, 0.0290,  ..., 0.0245, 0.0000, 0.0000],\n",
       "         [0.0273, 0.0309, 0.0230,  ..., 0.0183, 0.0394, 0.0000],\n",
       "         [0.0348, 0.0283, 0.0136,  ..., 0.0357, 0.0491, 0.0253]]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encoder_in = torch.randn(64,seq_len,d_model).to(device).to(dtype)\n",
    "encoder = Encoder(4,4,64).to(device)\n",
    "print(\"weights:\",sum(p.numel() for p in encoder.parameters() if p.requires_grad))\n",
    "start = time.time()\n",
    "out = encoder(encoder_in)\n",
    "end = time.time()\n",
    "print(f'time: {end - start}')\n",
    "print(out.shape)\n",
    "print(out)\n",
    "del out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = np.array([[1,0],[1,1]])\n",
    "b = a * np.array([[-np.inf,-np.inf],[-np.inf,-np.inf]])\n",
    "b[b == float('nan')] = 0\n",
    "torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones((5,5))\n",
    "b = np.triu(a, k=1) \n",
    "#b[b==1] = 0\n",
    "torch.tensor(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.triu(a, k=1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
