{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from flair.data import Sentence\n",
    "from flair.data import Token\n",
    "from flair.embeddings import WordEmbeddings\n",
    "%matplotlib inline\n",
    "\n",
    "# gpu\n",
    "device = torch.device(\"cuda:0\")\n",
    "dtype = torch.float\n",
    "\n",
    "input_language = 'de'\n",
    "output_language = 'en'\n",
    "embedding_model_en = WordEmbeddings('en')\n",
    "embedding_model_de = WordEmbeddings('de')\n",
    "\n",
    "# embedding size of fasttext models\n",
    "d_model = 300 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "df = pd.read_csv('data\\deu-eng\\pairs.txt', delimiter='\\t',\n",
    "                    usecols=[0,1],encoding='utf-8',names=['en','de'])\n",
    "\n",
    "number_sentences = 224351 # decrease max sequence length\n",
    "en = df['en'][0:number_sentences]\n",
    "de = df['de'][0:number_sentences]\n",
    "\n",
    "print(f'{en[224350]} --- {de[224350]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make vocabulary dict for en and de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len_en = -1\n",
    "max_sentence_len_de = -1\n",
    "\n",
    "en_set = set()\n",
    "for s in en:\n",
    "    \n",
    "    sentence = Sentence(s)\n",
    "    max_sentence_len_en = np.maximum(max_sentence_len_en, len(sentence))\n",
    "    \n",
    "    for token in sentence:\n",
    "        en_set.add(token.text)\n",
    "        \n",
    "de_set = set()\n",
    "for s in de:\n",
    "    \n",
    "    sentence = Sentence(s)\n",
    "    max_sentence_len_de = np.maximum(max_sentence_len_de, len(sentence))\n",
    "    \n",
    "    for token in sentence:\n",
    "        de_set.add(token.text)\n",
    "        \n",
    "en_dict = {}\n",
    "en_dict[\"vocab_size\"] = len(en_set)\n",
    "en_dict[\"max_sentence_len\"] = max_sentence_len_en # without <SOS> or <EOS>\n",
    "en_dict[\"<EOS>\"] = en_dict[\"vocab_size\"]\n",
    "en_dict[en_dict[\"vocab_size\"]] = \"<EOS>\"\n",
    "for i, token in enumerate(list(en_set)):\n",
    "    en_dict[i] = token\n",
    "    en_dict[token] = i\n",
    "    \n",
    "de_dict = {}\n",
    "de_dict[\"vocab_size\"] = len(de_set)\n",
    "de_dict[\"max_sentence_len\"] = max_sentence_len_de # without <SOS> or <EOS>\n",
    "de_dict[\"<EOS>\"] = de_dict[\"vocab_size\"]\n",
    "de_dict[de_dict[\"vocab_size\"]] = \"<EOS>\"\n",
    "for i, token in enumerate(list(de_set)):\n",
    "    de_dict[i] = token\n",
    "    de_dict[token] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocabulary en: \",len(en_set),\" | max sentence length: \", max_sentence_len_en)\n",
    "print(\"Vocabulary de: \",len(de_set),\" | max sentence length: \", max_sentence_len_de)\n",
    "filename = 'D:\\Transformer\\\\vocab_en.data'\n",
    "joblib.dump(en_dict, filename)\n",
    "filename = 'D:\\Transformer\\\\vocab_de.data'\n",
    "joblib.dump(de_dict, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create positional encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_mask = [2*i for i in range(d_model//2)]\n",
    "cosine_mask = [2*i+1 for i in range(d_model//2)]\n",
    "i = torch.tensor([i for i in range(d_model//2)]).to(device).to(dtype)\n",
    "\n",
    "# pre compute positional encodings for 50 tokens\n",
    "positional_encodings = []\n",
    "for position in range(50):\n",
    "    \n",
    "    sine = torch.sin( position / ( 10000 ** (2*i/d_model) ) )\n",
    "    cosine = torch.cos( position / ( 10000 ** (2*i/d_model) ) )\n",
    "    \n",
    "    position_enc = torch.zeros(d_model).to(device)\n",
    "    position_enc[sine_mask] = sine\n",
    "    position_enc[cosine_mask] = cosine\n",
    "\n",
    "    positional_encodings.append(position_enc)\n",
    "\n",
    "# return pre computed pos encoding\n",
    "def positional_encoding(sentence):\n",
    "    num_tokens = len(sentence)\n",
    "    return positional_encodings[0:num_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = Sentence('This is a test a b c r g.',language_code='en')\n",
    "enc_list = positional_encoding(s)\n",
    "enc = np.array([token_enc.cpu().numpy() for token_enc in enc_list])\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.pcolormesh(enc, cmap='viridis')\n",
    "plt.xlabel('Embedding Dimensions')\n",
    "plt.ylabel('Token Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make dataset (without zero padding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(sentence, position_dict):\n",
    "    \n",
    "    # cross entropy loss encoding for pytorch\n",
    "    # save only index (softmax output)\n",
    "    targets = []\n",
    "    \n",
    "    for token in sentence:\n",
    "        \n",
    "        if token.text == '<SOS>':\n",
    "            continue\n",
    "        \n",
    "        targets.append(position_dict[token.text])\n",
    "    \n",
    "    # add EOS token as last target\n",
    "    targets.append(position_dict['<EOS>'])\n",
    "    \n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = []\n",
    "for line in range(len(de)//batch_size//5):\n",
    "\n",
    "    sentences_en = []\n",
    "    for s in en[line*batch_size:line*batch_size+batch_size]:\n",
    "        \n",
    "        sentence = Sentence(s, language_code='en')\n",
    "        \n",
    "        if input_language == 'en': # add <EOS> token\n",
    "            sentence.add_token(\"<EOS>\")\n",
    "        else: # add <SOS> used for shifting ouput by one step\n",
    "            sentence.tokens = [Token(\"<SOS>\")] + sentence.tokens\n",
    "            \n",
    "        sentences_en.append(sentence)\n",
    "        \n",
    "    sentences_de = []\n",
    "    for s in de[line*batch_size:line*batch_size+batch_size]:\n",
    "        \n",
    "        sentence = Sentence(s, language_code='de')\n",
    "        \n",
    "        if input_language == 'de': # add <EOS> token\n",
    "            sentence.add_token(\"<EOS>\")\n",
    "        else: # add <SOS> used for shifting ouput by one step \n",
    "            sentence.tokens = [Token(\"<SOS>\")] + sentence.tokens\n",
    "            \n",
    "        sentences_de.append(sentence)\n",
    "        \n",
    "    embedding_model_en.embed(sentences_en)\n",
    "    embedding_model_de.embed(sentences_de)\n",
    "    \n",
    "    for sentence_en, sentence_de in zip(sentences_en,sentences_de):\n",
    "        \n",
    "        pos_enc_en = positional_encoding(sentence_en)\n",
    "        pos_enc_de = positional_encoding(sentence_de)\n",
    "        \n",
    "        en_embedding = torch.stack([token.embedding + pos_enc_en[i] for i,token in enumerate(sentence_en)])\n",
    "        de_embedding = torch.stack([token.embedding + pos_enc_de[i] for i,token in enumerate(sentence_de)])\n",
    "        \n",
    "        en_tokens = [token.text for token in sentence_en]\n",
    "        de_tokens = [token.text for token in sentence_de]\n",
    "        \n",
    "        target = make_target(sentence_en, en_dict) if output_language == 'en' else make_target(sentence_de, de_dict)\n",
    "        \n",
    "        if output_language ==  'en':\n",
    "            dataset.append([de_embedding.cpu(), en_embedding.cpu(), target])\n",
    "        else:\n",
    "            dataset.append([en_embedding.cpu(), de_embedding.cpu(), target])\n",
    "            \n",
    "    if line%100 == 0:\n",
    "        print(f'{line} / {len(df)//batch_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'D:\\Transformer\\{input_language}_to_{output_language}.data'\n",
    "joblib.dump(dataset, filename)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
